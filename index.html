<!DOCTYPE html>
<!-- saved from url=(0026)https://dingmyu.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <title>Tianxing Chen - 陈天行</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="./files/font-awesome.min.css">
  <link href="./files/pure-min.css" rel="stylesheet">
  <link href="./files/syntax.css" rel="stylesheet">
  <link href="./files/typora-wiki.css" rel="stylesheet">
  <link rel="icon" href="https://dingmyu.github.io/assets/img/favicon-16.png" sizes="16x16">
  <link rel="icon" href="https://dingmyu.github.io/assets/img/favicon-32.png" sizes="32x32">
  <link rel="icon" href="https://dingmyu.github.io/assets/img/favicon-48.png" sizes="48x48">
  <link rel="icon" href="https://dingmyu.github.io/assets/img/favicon-64.png" sizes="64x64">
  <link rel="icon" href="https://dingmyu.github.io/assets/img/favicon-128.png" sizes="128x128">
<script type="text/javascript" src="./files/jquery-1.12.4.min.js"></script><link type="text/css" rel="stylesheet" charset="UTF-8" href="./files/m=el_main_css"></head>

<body data-new-gr-c-s-check-loaded="14.1149.0" data-gr-ext-installed="">
  <style type="text/css">

    body {
      font-family: "Merriweather", "PT Serif", Georgia, "Times New Roman", "STSong", Serif;
      font-weight: 300;
    }

    h1, h2, h3, h4, h5, h6 {
      color: #2E2E2E;
      line-height: 1.15em;
      font-family: 'Open Sans','Hiragino Sans GB','Microsoft YaHei','WenQuanYi Micro Hei',sans-serif;
      text-rendering: geometricPrecision;
    }

/****************************************************************************/
/*SlideBar*/
.recent-work {
    height: auto;
    width: 102%;
    padding-bottom: 15px;
    overflow-x: scroll;
    overflow-y: hidden;
    margin: 0 auto 0 0;
}

.slider {
    width: 173%;
}

.slider a:hover {
    border: none
}

.slider img {
    height: auto;
    border-radius: 10px;
    margin-right: 20px;
}

.recent-work::-webkit-scrollbar {
    background: none;
    height: 8px;
}

.recent-work::-webkit-scrollbar-track {
    background: #eee;
    border-radius: 4px;
    margin-left: 240px;
    margin-right: 240px;
}

.recent-work::-webkit-scrollbar-thumb {
    background: #ccc;
    border-radius: 4px;
}

.recent-work::-webkit-scrollbar-thumb:hover {
    background: #999;
}
/*<div class="recent-work">
    <div class="slider">
        <a href="https://dingmyu.github.io/physion_v2/"><img src="/media/teaser/23cogsci_physion++.gif"></a>
        <a href="https://dingmyu.github.io/ecl/"><img src="/media/teaser/22corl_ECL.png"></a>
    </div>
</div>*/
/****************************************************************************/
/*https://codepen.io/rperry1886/pen/KKwbQNP*/
    .grid-container {
      columns: 7 150px;
      /*columns: 3 150px;*/
      /*columns: 3 auto;*/
      column-gap: 1.5rem;
      width: 95%;
      margin: 0 0 0 0;
    }
    .grid-container div {
      /*width: auto;*/
      /*width: 150px;*/
      /*margin: 0 1.5rem 1.5rem 0;*/
      margin: 0 0.5rem 0.5rem 0;
      display: inline-block;
      width: 100%;
      border: solid 2px black;
      padding: 5px;
      box-shadow: 5px 5px 5px rgba(0, 0, 0, 0.5);
      border-radius: 5px;
      transition: all 0.25s ease-in-out;

      overflow: hidden;
      /*display: block;*/
      position: relative;
      text-align: center;
    }
/*    .grid-container div:hover img {
      filter: grayscale(0);
    }*/
    .grid-container div:hover {
      border-color: coral;
    }
    .grid-container div img {
      width: 100%;
      /*filter: grayscale(100%);*/
      border-radius: 5px;
      transition: all 0.25s ease-in-out;
      margin: auto;
    }
    .grid-container div p {
      /*margin: 5px 0;*/
      margin: 0px 0;
      padding: 0;
      text-align: center;
      font-style: italic;
      font-family: sans-serif;
      font-size:15px;
    }
/****************************************************************************/

  	#pagecontainer {
  		background-color: #fff;
  		min-height: calc(100vh - 510px);
  	}

    #post-content {
      padding-top: 36px;
      border-top: 1px dashed #999999;
      line-height: 1.6rem;
      padding-bottom: 100px;
    }

    #post-content>blockquote:first-child {
      padding-top: 0.4em;
      padding-bottom: 0.4em;
      margin-bottom:2em;
      margin-left: 0;
    }

  	.nav {
      border-bottom: none;
      height: auto;
    }

  	.inner-page {
  		width: 90%;
  		/*max-width: 800px;*/
      max-width: 1025px;
  		margin: auto;
  	}

  	h1 {
  		border-bottom: 0;
  	}

    #header {
      padding-bottom: 32px;
    }

    #header h1 {
      text-align: center;
      white-space: pre-wrap;
      width: 90%;
      margin: 81px auto 48px auto;
    }

    h1 {
      font-size: 3.6rem;
      letter-spacing: -2px;
      text-indent: -3px;
    }

    h2 {
      margin-top: 2em;
    }

    .pure-menu-active>.pure-menu-link, .pure-menu-link:hover, .pure-menu-link:focus {
        background-color: initial;
        border: #777 solid 1px;
    }

    .post-meta {
      font-style: italic;
      text-align: center;
      color: #7a7a7a;
    }

    div.highlighter-rouge {
      padding: 8px;
      border: 1px #777 solid;
      line-height: 1.2rem;
      font-family: Menlo, Monaco, "Courier New", monospace;
    }

    div.highlighter-rouge pre{
      font-size: 0.95em;
      overflow: hidden;
      word-break: break-word;
      white-space: pre-wrap;
    }

    a {
      color: #463F5C;
      text-decoration: underline;
    }
  </style>

  <header class="pure-menu pure-menu-horizontal nav post-title" id="header">
    <h1>Tianxing Chen - 陈天行</h1>
  </header>

  <div id="pagecontainer" class="container">
    <div id="post-content" class="inner-page">
        <div id="profile">
<style>
.avatar {
  vertical-align: middle;
  width: 180px;
  height: 270px;
  border-radius: 10%;
  float: left;
  padding: 16px;
  border:  0 solid black;
}
</style>
<img class="avatar" src="./files/avatar.jpg" alt="Tianxing Chen">
</div>
<p>我当前正在深圳大学攻读计算机专业学士学位，同时在上海人工智能实验室(上海AI Lab)进行科研研究。未来计划攻读博士学位。</p>
<p>我是深大本科生最高荣誉获得者(top 0.0067%)、CCF 优秀大学生(全国99人,2023)；获得过ACM-ICPC区域赛银牌、RAICOM机器人开发者大赛全国亚军、蓝桥杯全国一等奖等16项国家级竞赛奖项；担任着深大ACM算法集训队队长、广东省“青年百千万工程”负责人等；斩获奖学金17项, 共计CNY150,000。</p>
<p>我的研究兴趣主要集中于基于Reinforcement Learning(RL)的行为规划在机器人、自动驾驶场景上的应用, 同时对LLM for Robotics, Embodied AI以及Diffusion Model for RL非常感兴趣</p>
<p>myding at berkeley dot edu &nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?user=w4yTWwoAAAAJ&amp;hl=en">[Google Scholar]</a></p>

<h2 id="research-highlights">Research Highlights</h2>

<p>My long-term research goal is to build embodied agents that can reason about and interact effectively with the physical world.</p>
<ul>
  <li><strong>Embodied AI</strong>: robot learning, LLM planner, physical simulation, commonsense reasoning</li>
  <li><strong>General-purpose Models</strong>: vision-language foundation models, self-supervised learning, 3D vision</li>
</ul>

<div class="recent-work">
<div class="slider">
<div class="grid-container" style="margin-bottom:-20px;margin-top:auto;">

  <div>
  <a href="https://dingmyu.github.io/ecl/">
    <img class="grid-item grid-item-1" src="./files/22corl_ECL.png" style="width:100%;margin-top:2.1%;margin-bottom:2.1%;" alt=""></a>
    <p style="font-size:12px;">CoRL22: Embodied Concept Learner</p>
  </div>

  <div>
  <a href="https://dingmyu.github.io/physion_v2/">
    <img class="grid-item grid-item-2" src="./files/23cogsci_physion++.gif" style="width:100%;margin-top:2.1%;margin-bottom:2.1%;" alt=""></a>
    <p style="font-size:12px;">NeurIPS23 Dataset: Physion++</p>
  </div>

<!-- ---------------- -->

  <div>
  <a href="https://object814.github.io/Task-Condition-With-LLM">
    <img class="grid-item grid-item-3.1" src="./files/llmtask1.gif" style="width:90%;margin-top:0%;margin-bottom:-1.3%;" alt="">
    <img class="grid-item grid-item-3.2" src="./files/llmtask2.gif" style="width:90%;margin-top:-1.3%;margin-bottom:-1.3%;" alt=""></a>
    <p style="font-size:12px;">Preprint: Long-Horizon Tasks with LLM</p>
  </div>

  <div>
  <a href="https://sites.google.com/view/llm-ad">
    <img class="grid-item grid-item-4" src="./files/languageMPC.gif" style="width:100%;margin-top:0%;margin-bottom:-1.3%;" alt=""></a>
    <p style="font-size:12px;">Preprint: LanguageMPC</p>
  </div>

<!-- ---------------- -->

  <div>
  <a href="https://sites.google.com/view/human-oriented-robot-learning">
    <img class="grid-item grid-item-5.1" src="./files/fanuc1.gif" style="width:45.2%;margin-top:0%;margin-bottom:-1.3%;" alt="">
    <img class="grid-item grid-item-5.2" src="./files/fanuc2.gif" style="width:45.2%;margin-top:0%;margin-bottom:-1.3%;" alt="">
    <img class="grid-item grid-item-5.3" src="https://dingmyu.github.io/media/teaser/fanuc3.gif" style="width:45.2%;margin-top:-1.3%;margin-bottom:-1.3%;" alt="">
    <img class="grid-item grid-item-5.4" src="https://dingmyu.github.io/media/teaser/fanuc4.gif" style="width:45.2%;margin-top:-1.3%;margin-bottom:-1.3%;" alt=""></a>
    <p style="font-size:12px;">Preprint: Human-oriented Manipulation</p>
  </div>

  <div>
  <a href="https://sites.google.com/view/ctrlformer-icml/">
    <img class="grid-item grid-item-6" src="https://dingmyu.github.io/media/teaser/22icml_CtrlFormer.gif" style="width:100%;margin-top:0%;margin-bottom:-1.3%;" alt=""></a>
    <p style="font-size:12px;">ICML22: CtrlFormer</p>
  </div>

<!-- ---------------- -->

  <div>
  <a href="https://embodiedgpt.github.io/">
    <img class="grid-item grid-item-7" src="./files/arxiv_embodiedGPT.png" style="width:70%;margin-top:0%;margin-bottom:-1.3%;" alt=""></a>
    <p style="font-size:12px;">NeurIPS23: EmbodiedGPT</p>
  </div>

  <div>
  <a href="https://ecforembodiedai.github.io/">
    <img class="grid-item grid-item-8" src="https://dingmyu.github.io/media/teaser/23cvpr_ec2.gif" style="width:70%;margin-top:0%;margin-bottom:-1.3%;" alt=""></a>
    <p style="font-size:12px;">CVPR23: EC for Embodied Control</p>
  </div>

  <div>
  <a href="https://adaptdiffuser.github.io/">
    <img class="grid-item grid-item-9.1" src="https://dingmyu.github.io/media/teaser/23icml_adaptdiffuser_gif1.gif" style="width:29.4%;margin-top:0%;margin-bottom:-1.3%;" alt="">
    <img class="grid-item grid-item-9.2" src="https://dingmyu.github.io/media/teaser/23icml_adaptdiffuser_gif2.gif" style="width:28%;margin-top:0%;margin-bottom:-1.3%;" alt=""></a>
    <p style="font-size:12px;">ICML23: AdaptDiffuser</p>
  </div>

<!-- ---------------- -->

  <div>
  <a href="https://robotics-transformer-x.github.io/">
    <img class="grid-item grid-item-10" src="https://dingmyu.github.io/media/teaser/rtx_thumbnail.gif" style="width:100%;margin-top:0%;margin-bottom:-1.3%;" alt=""></a>
    <p style="font-size:12px;">Preprint: RT-X</p>
  </div>

  <div>
  <a href="https://tree-planner.github.io/">
    <img class="grid-item grid-item-11" src="https://dingmyu.github.io/media/teaser/tree-planner.png" style="width:100%;height:auto;" alt=""></a>
    <p style="font-size:12px;">Preprint: Tree-Planner</p>
  </div>

<!-- ---------------- -->

  <div>
  <a href="https://comphyreasoning.github.io/">
    <img class="grid-item grid-item-12" src="https://dingmyu.github.io/media/teaser/22neurips_ComPhy.gif" alt=""></a>
    <p style="font-size:12px;">NeurIPS22: ComPhy Benchmark</p>
  </div>

  <div>
  <a href="https://dingmyu.github.io/vrdp/">
    <img class="grid-item grid-item-13" src="https://dingmyu.github.io/media/teaser/21neurips_VRDP.gif" style="width:78.5%;height:auto;" alt=""></a>
    <p style="font-size:12px;">NeurIPS21: Reasoning with DiffPhysics</p>
  </div>

<!-- ---------------- -->

  <div>
  <a href="https://arxiv.org/pdf/1912.04799.pdf">
    <img class="grid-item grid-item-14" src="https://dingmyu.github.io/media/teaser/20cvpr_D4LCN.gif" style="width:100%;" alt=""></a>
    <p style="font-size:12px;">CVPR20: Depth-Guided 3D Det</p>
  </div>

  <div>
  <a href="https://dingmyu.github.io/davit/">
    <img class="grid-item grid-item-15" src="https://dingmyu.github.io/media/teaser/22eccv_DaViT1.png" style="width:72%;height:auto;" alt=""></a>
    <p style="font-size:12px;">ECCV22: DaViT</p>
  </div>
</div>
</div>
</div>

<h2 id="selected-publications-full-list">Selected Publications <a href="https://dingmyu.github.io/Full-Publications/">[Full List]</a></h2>

<p>&nbsp;&nbsp;&nbsp;&nbsp; († Corresponding author.)</p>

<!--
- Generalizable Long-Horizon Manipulations with Large Language Models
    - Haoyu Zhou, **Mingyu Ding**†, Weikun Peng, Masayoshi Tomizuka, Lin Shao†, Chuang Gan  
        `Preprint 2023` [[paper]](https://arxiv.org/pdf/2310.02264) [[project]](https://object814.github.io/Task-Condition-With-LLM)

- Open X-Embodiment: Robotic Learning Datasets and RT-X Models
    - Open X-Embodiment Collaboration  
        `Preprint 2023` [[paper]](https://robotics-transformer-x.github.io/paper.pdf) [[code]](https://console.cloud.google.com/storage/browser/gresearch/robotics/open_x_embodiment_and_rt_x_oss;tab=objects?prefix=&forceOnObjectsSortingFiltering=false) [[project]](https://robotics-transformer-x.github.io/)

- Human-oriented Representation Learning for Robotic Manipulation
    - Mingxiao Huo, **Mingyu Ding**†, Chenfeng Xu, Thomas Tian, Xinghao Zhu, Yao Mu, Lingfeng Sun, Masayoshi Tomizuka, Wei Zhan  
        `Preprint 2023` [[paper]](https://arxiv.org/pdf/2310.03023) [[project]](https://sites.google.com/view/human-oriented-robot-learning)

- Large Language Models as Decision Makers for Autonomous Driving
    - Hao Sha, Yao Mu, Yuxuan Jiang, Li Chen, Chenfeng Xu, Ping Luo, Shengbo Eben Li, Masayoshi Tomizuka, Wei Zhan, **Mingyu Ding**†  
        `Preprint 2023` [[paper]](https://arxiv.org/pdf/2310.03026) [[project]](https://sites.google.com/view/llm-ad)

- EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought
    - Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, **Mingyu Ding**†, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo†  
        `NeurIPS 2023` [[paper]](https://arxiv.org/pdf/2305.15021) [[project]](https://embodiedgpt.github.io/)

- Physion++: Evaluating Physical Scene Understanding that Requires Online Inference of Different Physical Properties
    - Hsiao-Yu Tung\*, **Mingyu Ding**\*, Zhenfang Chen, Daniel M. Bear, Chuang Gan, Joshua B. Tenenbaum, Daniel L. K. Yamins, Judith Fan, Kevin A. Smith  
        `NeurIPS dataset track 2023` [[paper]](https://arxiv.org/pdf/2306.15668) [[project]](https://dingmyu.github.io/physion_v2/)

- Towards Free Data Selection with General-Purpose Models
    - Yichen Xie, **Mingyu Ding**†, Masayoshi Tomizuka, Wei Zhan  
        `NeurIPS 2023` [[paper]](https://arxiv.org/pdf/2309.17342.pdf) [[code]](https://github.com/yichen928/FreeSel)

- Embodied Concept Learner: Self-supervised Learning of Concepts and Mapping through Instruction Following
    - **Mingyu Ding**, Yan Xu, Zhenfang Chen, David Daniel Cox, Ping Luo, Joshua B. Tenenbaum, Chuang Gan  
        `CoRL 2022` [[paper]](https://arxiv.org/pdf/2304.03767.pdf) [[code]](https://github.com/dingmyu/ECL) [[project]](http://ecl.csail.mit.edu/)

- DaViT: Dual Attention Vision Transformers
    - **Mingyu Ding**, Bin Xiao, Noel Codella, Ping Luo, Jingdong Wang, Lu Yuan  
        `ECCV 2022` [[paper]](https://arxiv.org/pdf/2204.03645.pdf) [[code]](https://github.com/dingmyu/davit) [[project]](https://dingmyu.github.io/davit/)

- Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language
    - **Mingyu Ding**, Zhenfang Chen, Tao Du, Ping Luo, Joshua B. Tenenbaum, Chuang Gan  
        `NeurIPS 2021` [[paper]](https://arxiv.org/pdf/2110.15358.pdf) [[code]](https://github.com/dingmyu/VRDP) [[project]](http://vrdp.csail.mit.edu/)

- CamNet: Coarse-to-Fine Retrieval for Camera Re-localization
    - **Mingyu Ding**, Zhe Wang, Jiankai Sun, Jianping Shi, Ping Luo  
        `ICCV 2019` [[paper]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_CamNet_Coarse-to-Fine_Retrieval_for_Camera_Re-Localization_ICCV_2019_paper.pdf) [[code]](https://github.com/dingmyu/CamNet)
-->

<ul>
  <li>EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought
    <ul>
      <li>Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, <strong>Mingyu Ding</strong>†, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo<br>
  <code class="language-plaintext highlighter-rouge">NeurIPS 2023</code> (<strong>Spotlight</strong>) <a href="https://arxiv.org/pdf/2305.15021">[paper]</a> <a href="https://embodiedgpt.github.io/">[project]</a></li>
    </ul>
  </li>
  <li>Towards Free Data Selection with General-Purpose Models
    <ul>
      <li>Yichen Xie, <strong>Mingyu Ding</strong>†, Masayoshi Tomizuka, Wei Zhan<br>
  <code class="language-plaintext highlighter-rouge">NeurIPS 2023</code> <a href="https://arxiv.org/pdf/2309.17342.pdf">[paper]</a> <a href="https://github.com/yichen928/FreeSel">[code]</a></li>
    </ul>
  </li>
  <li>Visual Dependency Transformers: Dependency Tree Emerges from Reversed Attention
    <ul>
      <li><strong>Mingyu Ding</strong>, Yikang Shen, Lijie Fan, Zhenfang Chen, Zitian Chen, Ping Luo, Joshua B. Tenenbaum, Chuang Gan<br>
  <code class="language-plaintext highlighter-rouge">CVPR 2023</code> <a href="https://arxiv.org/pdf/2304.03282.pdf">[paper]</a> <a href="https://github.com/dingmyu/DependencyViT">[code]</a></li>
    </ul>
  </li>
  <li>Embodied Concept Learner: Self-supervised Learning of Concepts and Mapping through Instruction Following
    <ul>
      <li><strong>Mingyu Ding</strong>, Yan Xu, Zhenfang Chen, David Daniel Cox, Ping Luo, Joshua B. Tenenbaum, Chuang Gan<br>
  <code class="language-plaintext highlighter-rouge">CoRL 2022</code> <a href="https://arxiv.org/pdf/2304.03767.pdf">[paper]</a> <a href="https://github.com/dingmyu/ECL">[code]</a> <a href="http://ecl.csail.mit.edu/">[project]</a></li>
    </ul>
  </li>
  <li>DaViT: Dual Attention Vision Transformers
    <ul>
      <li><strong>Mingyu Ding</strong>, Bin Xiao, Noel Codella, Ping Luo, Jingdong Wang, Lu Yuan<br>
  <code class="language-plaintext highlighter-rouge">ECCV 2022</code> <a href="https://arxiv.org/pdf/2204.03645.pdf">[paper]</a> <a href="https://github.com/dingmyu/davit">[code]</a> <a href="https://dingmyu.github.io/davit/">[project]</a></li>
    </ul>
  </li>
  <li>Learning Versatile Neural Architectures by Propagating Network Codes
    <ul>
      <li><strong>Mingyu Ding</strong>, Yuqi Huo, Haoyu Lu, Linjie Yang, Zhe Wang, Zhiwu Lu, Jingdong Wang, Ping Luo<br>
  <code class="language-plaintext highlighter-rouge">ICLR 2022</code> <a href="https://arxiv.org/pdf/2103.13253.pdf">[paper]</a> <a href="https://github.com/dingmyu/NCP">[code]</a> <a href="https://network-propagation.github.io/">[project]</a></li>
    </ul>
  </li>
  <li>Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language
    <ul>
      <li><strong>Mingyu Ding</strong>, Zhenfang Chen, Tao Du, Ping Luo, Joshua B. Tenenbaum, Chuang Gan<br>
  <code class="language-plaintext highlighter-rouge">NeurIPS 2021</code> <a href="https://arxiv.org/pdf/2110.15358.pdf">[paper]</a> <a href="https://github.com/dingmyu/VRDP">[code]</a> <a href="http://vrdp.csail.mit.edu/">[project]</a></li>
    </ul>
  </li>
  <li>HR-NAS: Searching Efficient High-Resolution Neural Architectures with Lightweight Transformers
    <ul>
      <li><strong>Mingyu Ding</strong>, Xiaochen Lian, Linjie Yang, Peng Wang, Xiaojie Jin, Zhiwu Lu, Ping Luo<br>
  <code class="language-plaintext highlighter-rouge">CVPR 2021</code> (<strong>Oral</strong>) <a href="https://arxiv.org/pdf/2106.06560.pdf">[paper]</a> <a href="https://github.com/dingmyu/HR-NAS">[code]</a></li>
    </ul>
  </li>
  <li>Learning Depth-Guided Convolutions for Monocular 3D Object Detection
    <ul>
      <li><strong>Mingyu Ding</strong>, Yuqi Huo, Hongwei Yi, Zhe Wang,  Jianping Shi, Zhiwu Lu, Ping Luo<br>
  <code class="language-plaintext highlighter-rouge">CVPR 2020</code> <a href="https://arxiv.org/pdf/1912.04799.pdf">[paper]</a> <a href="https://github.com/dingmyu/D4LCN">[code]</a></li>
    </ul>
  </li>
  <li>Every Frame Counts: Joint Learning of Video Segmentation and Optical Flow
    <ul>
      <li><strong>Mingyu Ding</strong>, Zhe Wang, Bolei Zhou, Jianping Shi, Zhiwu Lu, Ping Luo<br>
  <code class="language-plaintext highlighter-rouge">AAAI 2020</code> <a href="https://arxiv.org/pdf/1911.12739.pdf">[paper]</a></li>
    </ul>
  </li>
  <li>CamNet: Coarse-to-Fine Retrieval for Camera Re-localization
    <ul>
      <li><strong>Mingyu Ding</strong>, Zhe Wang, Jiankai Sun, Jianping Shi, Ping Luo<br>
  <code class="language-plaintext highlighter-rouge">ICCV 2019</code> <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_CamNet_Coarse-to-Fine_Retrieval_for_Camera_Re-Localization_ICCV_2019_paper.pdf">[paper]</a> <a href="https://github.com/dingmyu/CamNet">[code]</a></li>
    </ul>
  </li>
</ul>

<h2 id="selected-honors">Selected Honors</h2>
<ul>
  <li>2023 ME Rising Stars, UC Berkeley</li>
  <li>2023 CVPR Doctoral Consortium</li>
  <li><a href="https://inf.news/ne/tech/219c42176bc5c9a8b14e2c25a6d0cab6.html">2022 WAIC Rising Stars</a> (15 awardees globally each year)</li>
  <li><a href="https://inf.news/en/auto/76af93b31d4b3a749c3d808525e606ea.html">2021 Baidu Fellowship</a> (10 awardees globally each year)</li>
  <li><a href="https://www.msra.cn/zh-cn/connections/academic-programs/fellows#!%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95">2020 Microsoft Fellowship Nomination Award</a> (15 nominations in Asia)</li>
  <li>M. Braun Postgraduate Prize, University of Hong Kong</li>
  <li>Best Student Paper Runner-up Award in 25th International Conference on Neural Information Processing (ICONIP)</li>
  <li>Outstanding Graduate of Beijing</li>
  <li>National Scholarship</li>
</ul>

<h2 id="activities">Activities</h2>
<ul>
  <li>Conference Reviewer for ICML, ICLR, NeurIPS, CVPR, ICRA, ICCV, ECCV, AAAI, IROS, WACV, ACCV, ACMMM, IV</li>
  <li>Journal Reviewer for TPAMI, IJCV, TIP, TCSVT, TMM, TOMM, TITS, TIV, RA-L, ACM CSUR, Neurocomputing</li>
</ul>


    </div>
  </div>

  <footer id="post-footer" style="background-color: rgb(51, 51, 51); padding-top:30px; padding-bottom:0px; text-align:center;font-size:13px;">
      Last updated: October 2023
      <!-- <script type="text/javascript" id="clustrmaps" src="https://cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=223&amp;t=n&amp;d=QP52ZCIA588YIlOSLN7s3znfB9AmkkU1ZecCaEkveCk&amp;co=333333"></script> -->
  </footer>
  <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" id=""></script><div id="goog-gt-tt" class="VIpgJd-yAWNEb-L7lbkb skiptranslate" style="border-radius: 12px; margin: 0 0 0 -23px; padding: 0; font-family: &#39;Google Sans&#39;, Arial, sans-serif;" data-id=""><div id="goog-gt-vt" class="VIpgJd-yAWNEb-hvhgNd"><div class=" VIpgJd-yAWNEb-hvhgNd-l4eHX-i3jM8c"><img src="https://fonts.gstatic.com/s/i/productlogos/translate/v14/24px.svg" width="24" height="24" alt=""></div><div class=" VIpgJd-yAWNEb-hvhgNd-k77Iif-i3jM8c"><div class="VIpgJd-yAWNEb-hvhgNd-IuizWc" dir="ltr">Original text</div><div id="goog-gt-original-text" class="VIpgJd-yAWNEb-nVMfcd-fmcmS VIpgJd-yAWNEb-hvhgNd-axAV1"></div></div><div class="VIpgJd-yAWNEb-hvhgNd-N7Eqid ltr"><div class="VIpgJd-yAWNEb-hvhgNd-N7Eqid-B7I4Od ltr" dir="ltr"><div class="VIpgJd-yAWNEb-hvhgNd-UTujCb">Rate this translation</div><div class="VIpgJd-yAWNEb-hvhgNd-eO9mKe">Your feedback will be used to help improve Google Translate</div></div><div class="VIpgJd-yAWNEb-hvhgNd-xgov5 ltr"><button id="goog-gt-thumbUpButton" type="button" class="VIpgJd-yAWNEb-hvhgNd-bgm6sf" title="Good translation" aria-label="Good translation" aria-pressed="false"><span id="goog-gt-thumbUpIcon"><svg width="24" height="24" viewBox="0 0 24 24" focusable="false" class="VIpgJd-yAWNEb-hvhgNd-THI6Vb NMm5M"><path d="M21 7h-6.31l.95-4.57.03-.32c0-.41-.17-.79-.44-1.06L14.17 0S7.08 6.85 7 7H2v13h16c.83 0 1.54-.5 1.84-1.22l3.02-7.05c.09-.23.14-.47.14-.73V9c0-1.1-.9-2-2-2zM7 18H4V9h3v9zm14-7l-3 7H9V8l4.34-4.34L12 9h9v2z"></path></svg></span><span id="goog-gt-thumbUpIconFilled"><svg width="24" height="24" viewBox="0 0 24 24" focusable="false" class="VIpgJd-yAWNEb-hvhgNd-THI6Vb NMm5M"><path d="M21 7h-6.31l.95-4.57.03-.32c0-.41-.17-.79-.44-1.06L14.17 0S7.08 6.85 7 7v13h11c.83 0 1.54-.5 1.84-1.22l3.02-7.05c.09-.23.14-.47.14-.73V9c0-1.1-.9-2-2-2zM5 7H1v13h4V7z"></path></svg></span></button><button id="goog-gt-thumbDownButton" type="button" class="VIpgJd-yAWNEb-hvhgNd-bgm6sf" title="Poor translation" aria-label="Poor translation" aria-pressed="false"><span id="goog-gt-thumbDownIcon"><svg width="24" height="24" viewBox="0 0 24 24" focusable="false" class="VIpgJd-yAWNEb-hvhgNd-THI6Vb NMm5M"><path d="M3 17h6.31l-.95 4.57-.03.32c0 .41.17.79.44 1.06L9.83 24s7.09-6.85 7.17-7h5V4H6c-.83 0-1.54.5-1.84 1.22l-3.02 7.05c-.09.23-.14.47-.14.73v2c0 1.1.9 2 2 2zM17 6h3v9h-3V6zM3 13l3-7h9v10l-4.34 4.34L12 15H3v-2z"></path></svg></span><span id="goog-gt-thumbDownIconFilled"><svg width="24" height="24" viewBox="0 0 24 24" focusable="false" class="VIpgJd-yAWNEb-hvhgNd-THI6Vb NMm5M"><path d="M3 17h6.31l-.95 4.57-.03.32c0 .41.17.79.44 1.06L9.83 24s7.09-6.85 7.17-7V4H6c-.83 0-1.54.5-1.84 1.22l-3.02 7.05c-.09.23-.14.47-.14.73v2c0 1.1.9 2 2 2zm16 0h4V4h-4v13z"></path></svg></span></button></div></div><div id="goog-gt-votingHiddenPane" class="VIpgJd-yAWNEb-hvhgNd-aXYTce"><form id="goog-gt-votingForm" action="https://translate.googleapis.com/translate_voting?client=te_lib" method="post" target="votingFrame" class="VIpgJd-yAWNEb-hvhgNd-aXYTce"><input type="text" name="sl" id="goog-gt-votingInputSrcLang"><input type="text" name="tl" id="goog-gt-votingInputTrgLang"><input type="text" name="query" id="goog-gt-votingInputSrcText"><input type="text" name="gtrans" id="goog-gt-votingInputTrgText"><input type="text" name="vote" id="goog-gt-votingInputVote"></form><iframe name="votingFrame" frameborder="0" src="./files/saved_resource.html"></iframe></div></div></div> -->

<!-- 
</body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html> -->